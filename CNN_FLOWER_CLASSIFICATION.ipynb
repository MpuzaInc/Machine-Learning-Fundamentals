{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5ab2fe5-ce42-4ff1-816b-29bebc317bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4a6029-0ef3-47d6-965f-6ee640a8aba5",
   "metadata": {},
   "source": [
    "## install the following library to help split the dataset into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "272e18cf-a8b6-4986-8fb8-c1587e12107a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install split-folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab853398-78e0-4048-9f4d-f8f2441f1bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT THIS FOR THE FIRST TIME TO MAKE THOSE SPLITS BUT NOT NEEDED AFTER\n",
    "#import splitfolders\n",
    "# Split the dataset into train (80%) and test (20%)\n",
    "#splitfolders.ratio(\"flower_images\", output=\"flowers_split\", seed=1337, ratio=(.8, .2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "996e52c7-e5e1-4b86-a451-6e5bdce0fc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    shear_range=0.2,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    fill_mode='nearest')\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92e086bd-fabd-4b04-a20b-fbf7a24dab96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4008 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'flowers_split/train',\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7da50b44-9947-469e-a82f-f9c1ecdc1799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1002 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "val_generator = val_datagen.flow_from_directory(\n",
    "    'flowers_split/val',\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e1f0861-c01b-404b-a4e6-268769c82186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Lilly': 0, 'Lotus': 1, 'Orchid': 2, 'Sunflower': 3, 'Tulip': 4}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ee359b-b818-4b90-99bf-3b4fdd29fab7",
   "metadata": {},
   "source": [
    "### if class indice create a hidden zero index \".ipynb_checkpoints\", you can remove it using your CMD to remain with 5 classes instead of 6 using the following commented command in your CMD IF YOU USE WINDOWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29a180e6-6011-4c45-9758-a35ab362e5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for /d /r %i in (*.ipynb_checkpoints) do @if exist \"%i\" rd /s /q \"%i\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5098bf-060c-4a04-a9fd-c97ba48f4dbe",
   "metadata": {},
   "source": [
    "# Building CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aec5ce5a-b099-4cf1-abf7-acdec903dcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59811011-d563-49a7-814b-7fe1de89ccea",
   "metadata": {},
   "source": [
    "# first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f193ea6-908d-4b1f-acb9-fb9f42b47b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "cnn.add(Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=(128, 128, 3)))\n",
    "cnn.add(MaxPooling2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0deac8-4b65-4e21-a411-cce502f354bb",
   "metadata": {},
   "source": [
    "# second layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8fea6d4-5187-4f7e-9199-4e1f1c037309",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Conv2D(filters=64, kernel_size=3, activation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cc699f-a0fa-4983-9c6d-a8a6794c60e7",
   "metadata": {},
   "source": [
    "# third Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9da4b27c-dd4e-4be4-9181-33dfa45a24b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Conv2D(filters=128, kernel_size=3, activation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d418a991-de16-4577-bf57-d005efa330ba",
   "metadata": {},
   "source": [
    "# Flatten + Dense + Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e721806-bd52-4213-9be6-4fce780e90b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(units=128, activation='relu'))\n",
    "cnn.add(Dropout(0.5))\n",
    "cnn.add(Dense(units=5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b0f92c-f81b-4d35-9535-5dde5939c269",
   "metadata": {},
   "source": [
    "# compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8fed746b-0067-4f86-ae9a-ccc54156d5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c4bb01-85a0-449c-ac4e-f35f119327fe",
   "metadata": {},
   "source": [
    "# Train with Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "73cd9928-2a10-4151-ad0c-2f5725538b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "30bc578d-e75f-4287-b2c6-c796f8edafe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 981ms/step - accuracy: 0.3653 - loss: 1.4674 - val_accuracy: 0.5230 - val_loss: 1.1854\n",
      "Epoch 2/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 949ms/step - accuracy: 0.5150 - loss: 1.2112 - val_accuracy: 0.5569 - val_loss: 1.0959\n",
      "Epoch 3/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 880ms/step - accuracy: 0.5575 - loss: 1.1308 - val_accuracy: 0.5828 - val_loss: 1.0451\n",
      "Epoch 4/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 850ms/step - accuracy: 0.5799 - loss: 1.0719 - val_accuracy: 0.5818 - val_loss: 1.0213\n",
      "Epoch 5/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 876ms/step - accuracy: 0.5664 - loss: 1.0933 - val_accuracy: 0.6168 - val_loss: 0.9706\n",
      "Epoch 6/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 864ms/step - accuracy: 0.6069 - loss: 1.0064 - val_accuracy: 0.6218 - val_loss: 0.9439\n",
      "Epoch 7/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 866ms/step - accuracy: 0.6205 - loss: 0.9763 - val_accuracy: 0.6168 - val_loss: 0.9078\n",
      "Epoch 8/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 889ms/step - accuracy: 0.6380 - loss: 0.9411 - val_accuracy: 0.6277 - val_loss: 0.9236\n",
      "Epoch 9/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 875ms/step - accuracy: 0.6132 - loss: 0.9527 - val_accuracy: 0.6607 - val_loss: 0.8375\n",
      "Epoch 10/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 876ms/step - accuracy: 0.6463 - loss: 0.9005 - val_accuracy: 0.6826 - val_loss: 0.8394\n",
      "Epoch 11/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 892ms/step - accuracy: 0.6491 - loss: 0.9050 - val_accuracy: 0.6727 - val_loss: 0.8229\n",
      "Epoch 12/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 882ms/step - accuracy: 0.6517 - loss: 0.8988 - val_accuracy: 0.6926 - val_loss: 0.8192\n",
      "Epoch 13/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 890ms/step - accuracy: 0.6787 - loss: 0.8435 - val_accuracy: 0.6856 - val_loss: 0.8171\n",
      "Epoch 14/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 874ms/step - accuracy: 0.6734 - loss: 0.8391 - val_accuracy: 0.7036 - val_loss: 0.7591\n",
      "Epoch 15/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 867ms/step - accuracy: 0.6832 - loss: 0.8154 - val_accuracy: 0.6826 - val_loss: 0.7932\n",
      "Epoch 16/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 847ms/step - accuracy: 0.6829 - loss: 0.8180 - val_accuracy: 0.7176 - val_loss: 0.7403\n",
      "Epoch 17/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 865ms/step - accuracy: 0.6886 - loss: 0.8068 - val_accuracy: 0.7525 - val_loss: 0.6859\n",
      "Epoch 18/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 952ms/step - accuracy: 0.7152 - loss: 0.7658 - val_accuracy: 0.7395 - val_loss: 0.6892\n",
      "Epoch 19/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 874ms/step - accuracy: 0.7103 - loss: 0.7625 - val_accuracy: 0.7335 - val_loss: 0.7252\n",
      "Epoch 20/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 883ms/step - accuracy: 0.7125 - loss: 0.7357 - val_accuracy: 0.7206 - val_loss: 0.7023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x26d1953a490>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(train_generator, validation_data=val_generator, epochs=20, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "623d7498-c0b9-4576-b654-0ce7e14a7f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 359ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "# Get true labels\n",
    "true_labels = val_generator.classes\n",
    "\n",
    "# Get class indices\n",
    "class_indices = val_generator.class_indices\n",
    "labels = list(class_indices.keys())\n",
    "\n",
    "# Get predictions\n",
    "predictions = cnn.predict(val_generator)\n",
    "predicted_classes = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78d7a6cc-cf87-436a-bf97-5e844d533e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Lilly       0.61      0.70      0.66       200\n",
      "       Lotus       0.73      0.75      0.74       202\n",
      "      Orchid       0.73      0.67      0.70       200\n",
      "   Sunflower       0.90      0.97      0.94       200\n",
      "       Tulip       0.81      0.66      0.73       200\n",
      "\n",
      "    accuracy                           0.75      1002\n",
      "   macro avg       0.76      0.75      0.75      1002\n",
      "weighted avg       0.76      0.75      0.75      1002\n",
      "\n",
      "Confusion Matrix:\n",
      "[[141  19  19   8  13]\n",
      " [ 26 152  14   0  10]\n",
      " [ 38  17 134   3   8]\n",
      " [  4   1   0 195   0]\n",
      " [ 21  20  17  10 132]]\n"
     ]
    }
   ],
   "source": [
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_labels, predicted_classes, target_names=labels))\n",
    "\n",
    "# Optional: Confusion Matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(true_labels, predicted_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7c8fa86f-8f72-449e-aad2-708276206b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 271ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Load and preprocess the image\n",
    "test_image = image.load_img('testFlower.jpeg', target_size=(128, 128))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "test_image = test_image / 255.0\n",
    "\n",
    "result = cnn.predict(test_image)\n",
    "class_indices = {\n",
    "    0: 'Lilly',\n",
    "    1: 'Lotus',\n",
    "    2: 'Orchid',\n",
    "    3: 'Sunflower',\n",
    "    4: 'Tulip'\n",
    "}\n",
    "\n",
    "predicted_index = np.argmax(result)\n",
    "prediction = class_indices[predicted_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "88884cb1-d190-4c11-af85-2e2fe6ea592e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model predict this image to be : Sunflower\n"
     ]
    }
   ],
   "source": [
    "print(f\"The model predict this image to be : {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "33e12fba-18fb-4dc8-ac94-72f1eaf7013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "423c9e06-0192-4cff-8f46-af468aaa3eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.save(\"modelFlowers.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f93ebf00-d96e-4cc2-b558-399aa316f279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Load your trained model\n",
    "model = load_model(\"modelFlowers.keras\") \n",
    "\n",
    "# Define your class names\n",
    "class_indices = {\n",
    "    0: 'Lilly',\n",
    "    1: 'Lotus',\n",
    "    2: 'Orchid',\n",
    "    3: 'Sunflower',\n",
    "    4: 'Tulip'\n",
    "}\n",
    "\n",
    "# Define prediction function\n",
    "def predict_flower(img):\n",
    "    img = img.resize((128, 128))  # Resize to model input size\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = img / 255.0\n",
    "    prediction = model.predict(img)\n",
    "    index = np.argmax(prediction)\n",
    "    return class_indices[index]\n",
    "\n",
    "# Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=predict_flower,\n",
    "    inputs=gr.Image(type=\"pil\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Flower Image Classifier\",\n",
    "    description=\"Upload a flower image and get its predicted class\"\n",
    ")\n",
    "\n",
    "# Launch inline in notebook\n",
    "iface.launch(share=False, inline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f2fbac-8776-4c53-a6c4-cdc9d6a9dcc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
